# Bucket configuration
escrita:
  bucket_avro: landingzone-933447815926 # Sucess .avro file
  bucket_errors: datalake-errors-933447815926 # Error files
  destinationbucket: datalake-processed-files-933447815926 # files successfully processed
  prefixname: ORIGINALFILES/TESTS/ # the first folder in where to put them
#  topic: csn-logs # optional, the topic to write the logs in kafka, use this key only if you need kafka
# Extraction folder from the client bucket
dummy_files: # Name must be exactly as the folder of the file | Case sensitive
  class: CsvExcelExtractor # Optional, if empty, return default CsvExcelExtractor
  csv: # If extracting csv files, the header is MANDATORY, otherwise will throw exception
    header: 0 # MANDATORY, The line which becomes the column names
    encoding: UTF-8 # Optional, Defaults to UTF-8
    sep: "," # Optional, Defaults to ','
    low_memory: false # Optional, Defaults to false
  rename_type: index # Optional, formats accepted: normal, regex, index. Defaults to normal
  # Warning, when using regex or index, index or name errors can happen
  column_renames:   # Optional, use when need to rename columns
    # "(?i).*m.s\\s*de\\s*prg.*": data_demanda - regex
    # "0": data_demanda - index
    # data-demanda: data_demanda - normal
    "0": nome
    "1": sobrenome
    "2": endereco
    "3": bairro
    "4": estado
    "5": cep
  avro_schema:   # Avro schema that must be parsible
    type: record
    name: demanda
    fields:
    - name: nome
      type: string
    - name: sobrenome
      type:
      - 'null'
      - string
    - name: endereco
      type: string
    - name: bairro
      type: string
    - name: estado
      type:
      - 'null'
      - string
    - name: cep
      type: string
  name: demanda_  # prefix for the avro-file being saved in landingzone
  s3key: CSN/DEMANDA/ # the folder for the avro-file being saved in landingzone
  colunapath: url_path # Optional, the column to be created with the file key, defaults to url_path